{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d37a6bc0",
      "metadata": {
        "id": "d37a6bc0"
      },
      "source": [
        "# Assignment 1 - Binary Classification Evaluation Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1679a82",
      "metadata": {
        "id": "d1679a82"
      },
      "source": [
        "**Objective:**\n",
        "The objective of this assignment is to assess your understanding of fundamental concepts in model evaluation for machine learning tasks. This assignment covers topics discussed in the first half of the course, including key evaluation metrics, confusion matrices, ROC curves, and Precision-Recall curves.\n",
        "Instructions:\n",
        "\n",
        "1. Theory Questions:\n",
        "Answer the following theoretical questions:\n",
        "\n",
        "    1. Explain the limitations of accuracy as an evaluation metric in imbalanced datasets. How does accuracy behave when classes are heavily skewed, and why might it provide misleading results?\n",
        "    2. Describe the purpose and interpretation of a confusion matrix. How does it help in assessing a classification model's performance?\n",
        "    3. Explain the concept of ROC curves. What does each point on an ROC curve represent? How is the area under the ROC curve (AUC-ROC) calculated?\n",
        "    4. Compare and contrast the advantages and disadvantages of ROC curves and Precision-Recall curves. In what scenarios would you prefer to use one over the other, and why?\n",
        "\n",
        "2. Practical Exercises:\n",
        "* Implement Python code to calculate the following evaluation metrics for a given binary classification problem: Log Loss\n",
        "* Select the best metric for an applied scenario\n",
        "\n",
        "**Submission Guidelines:**\n",
        "* Submit your responses to the theory questions in a neatly organized markdown.\n",
        "* Include your Python code for the practical exercise.\n",
        "* Submit your assignment as a single `.ipynb` file named `MY NAME Assignment 1 - Log Loss` via the course submission platform (slack)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e58864fe",
      "metadata": {
        "id": "e58864fe"
      },
      "source": [
        "## Part 1: Theory Questions (20 points)\n",
        "Provide your answers here:\n",
        "\n",
        "### 1. Limitations of Accuracy as an Evaluation Metric in Imbalanced Datasets\n",
        "\n",
        "Accuracy is limited in imbalanced datasets because it simply measures the proportion of correct predictions among all predictions, without considering class distribution. When classes are heavily skewed, accuracy becomes misleading for several reasons:\n",
        "\n",
        "- The majority class dominates the accuracy calculation, causing the metric to be insensitive to performance on minority classes\n",
        "- A naive classifier that always predicts the majority class will achieve high accuracy without providing any useful information about minority classes\n",
        "- Accuracy fails to reflect the model's ability to correctly identify positive cases in scenarios where the positive class is rare but important (like fraud detection or disease diagnosis)\n",
        "\n",
        "For example, in a dataset with 98% negative cases and 2% positive cases, a model that always predicts \"negative\" would achieve 98% accuracy while completely failing to detect any positive cases.\n",
        "\n",
        "### 2. Purpose and Interpretation of a Confusion Matrix\n",
        "\n",
        "A confusion matrix is a table that describes the performance of a classification model by showing the counts of:\n",
        "- True Positives (TP): Correctly predicted positive cases\n",
        "- True Negatives (TN): Correctly predicted negative cases\n",
        "- False Positives (FP): Negative cases incorrectly predicted as positive (Type I error)\n",
        "- False Negatives (FN): Positive cases incorrectly predicted as negative (Type II error)\n",
        "\n",
        "The matrix helps assess model performance by enabling the calculation of various metrics:\n",
        "- Accuracy: (TP + TN) / (TP + TN + FP + FN)\n",
        "- Precision: TP / (TP + FP)\n",
        "- Recall/Sensitivity: TP / (TP + FN)\n",
        "- Specificity: TN / (TN + FP)\n",
        "- F1-Score: Harmonic mean of precision and recall\n",
        "\n",
        "Confusion matrices provide a comprehensive view of model performance across all classes, revealing where the model excels and where it struggles, particularly in identifying specific error types.\n",
        "\n",
        "### 3. Concept of ROC Curves\n",
        "\n",
        "ROC (Receiver Operating Characteristic) curves plot the True Positive Rate (TPR, also called recall or sensitivity) against the False Positive Rate (FPR, or 1-specificity) at various classification thresholds.\n",
        "\n",
        "Each point on an ROC curve represents a specific threshold value for classifying predictions as positive or negative. Moving along the curve corresponds to changing this threshold:\n",
        "- Points toward the upper-left corner indicate better classification performance\n",
        "- The diagonal line represents random guessing\n",
        "\n",
        "The Area Under the ROC Curve (AUC-ROC) is calculated by integrating the ROC curve from x=0 to x=1. It quantifies the overall ability of the model to discriminate between positive and negative classes:\n",
        "- AUC-ROC of 1.0 indicates perfect classification\n",
        "- AUC-ROC of 0.5 indicates performance equivalent to random guessing\n",
        "- AUC-ROC values below 0.5 suggest worse-than-random performance\n",
        "\n",
        "AUC-ROC can be interpreted as the probability that the model will rank a randomly chosen positive instance higher than a randomly chosen negative instance.\n",
        "\n",
        "### 4. ROC Curves vs. Precision-Recall Curves\n",
        "\n",
        "**ROC Curves:**\n",
        "- Advantages:\n",
        "  - Insensitive to class imbalance when comparing different models\n",
        "  - Provides a visual representation of the trade-off between TPR and FPR\n",
        "  - AUC-ROC is invariant to the decision threshold\n",
        "  \n",
        "- Disadvantages:\n",
        "  - Can be overly optimistic in highly imbalanced datasets\n",
        "  - FPR can be misleadingly low when the negative class is large\n",
        "  - Less informative when the focus is on positive class performance\n",
        "\n",
        "**Precision-Recall Curves:**\n",
        "- Advantages:\n",
        "  - Better suited for imbalanced datasets\n",
        "  - Focuses on the performance of the positive (typically minority) class\n",
        "  - More sensitive to improvements in positive class detection\n",
        "  - Directly shows the trade-off between precision and recall\n",
        "  \n",
        "- Disadvantages:\n",
        "  - More sensitive to changes in class distribution\n",
        "  - Not suitable for comparing models across datasets with different class distributions\n",
        "  - Does not account for true negative performance\n",
        "\n",
        "**When to prefer each:**\n",
        "\n",
        "Use ROC curves when:\n",
        "- Class distribution is relatively balanced\n",
        "- Both classes are equally important\n",
        "- You need to compare models across different datasets\n",
        "- False positive and false negative costs are similar\n",
        "\n",
        "Use Precision-Recall curves when:\n",
        "- Datasets are highly imbalanced\n",
        "- The positive/minority class is of primary interest\n",
        "- False positives are especially costly or concerning\n",
        "- Working in domains like information retrieval, anomaly detection, or medical diagnosis where finding rare positive cases is crucial"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b8c2adb",
      "metadata": {
        "id": "8b8c2adb"
      },
      "source": [
        "## Practicing Log Loss (25 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f4dca41",
      "metadata": {
        "id": "2f4dca41"
      },
      "source": [
        "**Objective:**\n",
        "The objective of this assignment is to deepen your understanding of log loss, also known as logarithmic loss or cross-entropy loss, and its application in evaluating the performance of classification models.\n",
        "\n",
        "**Instructions:**\n",
        "In this assignment, you will be given a set of binary classification predictions along with their corresponding actual class labels. Your task is to calculate the log loss for each prediction and then analyze the overall log loss performance of the model.\n",
        "\n",
        "**Dataset:**\n",
        "You are provided with a dataset containing the following information:\n",
        "\n",
        "Predicted probabilities for the positive class (ranging from 0 to 1) for a set of instances.\n",
        "Actual binary class labels (0 or 1) indicating whether the instance belongs to the positive class or not.\n",
        "\n",
        "**Assignment Tasks:**\n",
        "1. Calculate the log loss for each instance in the dataset using the predicted probabilities and actual class labels.\n",
        "2. Summarize the individual log losses and compute the overall log loss performance for the model.\n",
        "3. Interpret the overall log loss value and analyze the model's performance. Discuss any insights or observations derived from the log loss analysis.\n",
        "\n",
        "\n",
        "**Dataset:**\n",
        "\n",
        "| Instance | Predicted Probability | Actual Label |\n",
        "|----------|------------------------|--------------|\n",
        "|    1     |          0.9           |       1      |\n",
        "|    2     |          0.3           |       0      |\n",
        "|    3     |          0.6           |       1      |\n",
        "|    4     |          0.8           |       0      |\n",
        "|    5     |          0.1           |       1      |\n",
        "\n",
        "\n",
        "**Grading Criteria:**\n",
        "\n",
        "* Correctness of log loss calculations.\n",
        "* Clarity and completeness of the analysis.\n",
        "* Insights derived from the log loss interpretation.\n",
        "* Overall presentation and adherence to submission guidelines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3dad6832",
      "metadata": {
        "id": "3dad6832",
        "outputId": "cde5497f-343a-4eb0-97a5-a75413d99d77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Instance  Predicted Probability  Actual Label\n",
            "0         1                    0.9             1\n",
            "1         2                    0.3             0\n",
            "2         3                    0.6             1\n",
            "3         4                    0.8             0\n",
            "4         5                    0.1             1\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame with the dataset\n",
        "data = {\n",
        "    'Instance': [1, 2, 3, 4, 5],\n",
        "    'Predicted Probability': [0.9, 0.3, 0.6, 0.8, 0.1],\n",
        "    'Actual Label': [1, 0, 1, 0, 1]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9be2a3c3",
      "metadata": {
        "id": "9be2a3c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "454f78fe-e24d-4aa1-f4e5-a91ef566913c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log Loss Values: [np.float64(0.10536051565782628), np.float64(0.35667494393873245), np.float64(0.5108256237659907), np.float64(1.6094379124341005), np.float64(2.3025850929940455)]\n",
            "Overall Log Loss: 0.976976817758139\n"
          ]
        }
      ],
      "source": [
        "# Interpret the log loss\n",
        "import numpy as np\n",
        "\n",
        "def log_loss(y_true, y_pred):\n",
        "       \"\"\"Calculates the log loss.\n",
        "\n",
        "       Args:\n",
        "           y_true: The true binary labels (0 or 1).\n",
        "           y_pred: The predicted probabilities for the positive class (between 0 and 1).\n",
        "\n",
        "       Returns:\n",
        "           The log loss value.\n",
        "       \"\"\"\n",
        "       epsilon = 1e-15  # Small value to avoid log(0)\n",
        "       y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip predictions to avoid extreme values\n",
        "       loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "       return loss\n",
        "predicted_probabilities = df['Predicted Probability']\n",
        "actual_labels = df['Actual Label']\n",
        "log_loss_values = [log_loss(y_true, y_pred) for y_true, y_pred in zip(actual_labels, predicted_probabilities)]\n",
        "overall_log_loss = np.mean(log_loss_values)\n",
        "print(f\"Log Loss Values: {log_loss_values}\")\n",
        "print(f\"Overall Log Loss: {overall_log_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaa92db1",
      "metadata": {
        "id": "eaa92db1"
      },
      "source": [
        "*Question: Interpret the log loss above. How would it change if the predicted probability for instance 0 changed from 0.9 to 0.6? Why?*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2abec414",
      "metadata": {
        "id": "2abec414"
      },
      "source": [
        "*Your answer:*  Overall Log Loss (0.976976817758139): This value represents the average log loss across all instances in your dataset. A lower log loss indicates better model performance. In this case, the overall log loss is relatively high, suggesting that the model's predictions have room for improvement.\n",
        "\n",
        "Individual Log Loss Values: The list Log Loss Values provides the log loss for each instance separately. Let's examine them:\n",
        "\n",
        "Instance 1 (0.10536051565782628): This low log loss indicates a confident and accurate prediction for this instance. The model predicted a high probability (0.9) for the positive class, which aligned with the actual label (1).\n",
        "Instance 2 (0.35667494393873245): This moderate log loss suggests a reasonably good prediction. The model predicted a lower probability (0.3) for the positive class, which was correct as the actual label was (0).\n",
        "Instance 3 (0.5108256237659907): This log loss is slightly higher, indicating a less confident prediction. The model predicted a moderate probability (0.6) for the positive class, which was correct but not as certain as in Instance 1.\n",
        "Instance 4 (1.6094379124341005): This high log loss indicates a poor prediction. The model predicted a high probability (0.8) for the positive class, but the actual label was (0), resulting in a significant penalty.\n",
        "Instance 5 (2.3025850929940455): This very high log loss signifies a severely incorrect prediction. The model predicted a low probability (0.1) for the positive class, while the actual label was (1), leading to a substantial penalty.\n",
        "\n",
        "If the predicted probability for Instance 1 changed from 0.9 to 0.6, here's how the log loss would be affected:\n",
        "\n",
        "Increased Log Loss for Instance 1: The log loss for Instance 1 would increase because the model's prediction becomes less confident. Even though the predicted probability (0.6) is still greater than 0.5 and would likely result in a correct classification, it is further from the true label (1) compared to the original prediction (0.9). This increased deviation from the true label leads to a higher penalty in the log loss calculation.\n",
        "\n",
        "Increased Overall Log Loss: The overall log loss would also increase because the log loss for Instance 1 has increased. Since the overall log loss is the average of individual log losses, any increase in an individual log loss will contribute to a higher overall log loss."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "972c7485",
      "metadata": {
        "id": "972c7485"
      },
      "source": [
        "*Question: Why might you select log loss over precision, recall, or accuracy (in the context of any problem, not this one specifically)?*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88b26ee2",
      "metadata": {
        "id": "88b26ee2"
      },
      "source": [
        "*Your answer:*\n",
        "When prediction confidence is crucial: If you need to understand how confident your model is in its predictions, log loss is a good choice. For example, in medical diagnosis or financial fraud detection, it's important to know not only whether a prediction is correct but also how certain the model is about it.\n",
        "\n",
        "When dealing with probabilistic models: If your model outputs probabilities rather than just class labels (e.g., logistic regression, neural networks), log loss is a natural choice for evaluation. It directly works with the predicted probabilities, providing a more comprehensive assessment of the model's performance.\n",
        "\n",
        "When optimizing model training: Log loss is often used as the objective function during model training, as it's a continuous and differentiable function that can be effectively minimized by optimization algorithms.\n",
        "\n",
        "When dealing with imbalanced datasets: In cases of class imbalance, log loss can be a more robust metric than accuracy. It penalizes models that simply predict the majority class, encouraging them to learn patterns that can accurately identify the minority class as well.\n",
        "\n",
        "When evaluating ranking models: Log loss is commonly used in information retrieval and ranking tasks where the goal is to order items based on their relevance. It can assess the model's ability to rank positive instances higher than negative ones."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a69ed6d7",
      "metadata": {
        "id": "a69ed6d7"
      },
      "source": [
        "## Application Scenario: Select a Metric (55 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "791158c2",
      "metadata": {
        "id": "791158c2"
      },
      "source": [
        "**Application Scenario: Fraud Detection System**\n",
        "\n",
        "You are working as a data scientist for a financial institution that wants to develop a fraud detection system to identify potentially fraudulent transactions. The dataset contains information about various transactions, including transaction amount, merchant ID, and transaction type. Your task is to build a machine learning model to classify transactions as either fraudulent or non-fraudulent.\n",
        "\n",
        "**Problem Description:**\n",
        "\n",
        "* Dataset: The dataset consists of historical transaction data, with labels indicating whether each transaction was fraudulent or not.\n",
        "* Class Distribution: The dataset is mostly non-fraudulant cases, with a small percentage of transactions being fraudulent compared to legitimate transactions.\n",
        "* Objective: The objective is to develop a fraud detection model that minimizes false negatives (fraudulent transactions incorrectly classified as non-fraudulent) while maintaining a reasonable level of precision.\n",
        "\n",
        "**Stakeholder Requirements:**\n",
        "Given the nature of the problem, it is crucial to prioritize recall (sensitivity) to ensure that as many fraudulent transactions as possible are detected. However, precision is also important to minimize false positives and avoid unnecessary investigations of legitimate transactions. Minimizing false negatives (missing fraudulent transactions) is of utmost importance.\n",
        "\n",
        "**Task:**\n",
        "Your task is to develop Python code to evaluate the performance of different machine learning models using various evaluation metrics, including accuracy, precision, recall, and F2 score. *Select the evaluation metric that best suits the problem and explain your choice*.\n",
        "\n",
        "**Additional Guidelines:**\n",
        "* You should preprocess the dataset as needed and split it into training and testing sets.\n",
        "* Implement machine learning models of your choice (e.g., logistic regression, random forest) and evaluate their performance.\n",
        "* Use appropriate evaluation metrics for binary classification tasks.\n",
        "* Discuss the rationale behind your choice of evaluation metric and how it aligns with the problem requirements.\n",
        "* Present your findings and recommendations for selecting the best model based on the chosen evaluation metric."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc4f0e46",
      "metadata": {
        "id": "dc4f0e46"
      },
      "source": [
        "**Dataset Sample:**\n",
        "\n",
        "| Transaction ID | Transaction Amount | Merchant ID | Transaction Type | Fraudulent |\n",
        "|----------------|--------------------|-------------|------------------|------------|\n",
        "| 1              | 1000               | M123        | Online Purchase  | 0          |\n",
        "| 2              | 500                | M456        | ATM Withdrawal   | 0          |\n",
        "| 3              | 2000               | M789        | Online Purchase  | 1          |\n",
        "| 4              | 1500               | M123        | POS Transaction  | 0          |\n",
        "| 5              | 800                | M456        | Online Purchase  | 0          |\n",
        "| 6              | 3000               | M789        | ATM Withdrawal   | 1          |\n",
        "\n",
        "* Transaction ID: Unique identifier for each transaction.\n",
        "* Transaction Amount: The amount of money involved in the transaction.\n",
        "* Merchant ID: Identifier for the merchant involved in the transaction.\n",
        "* Transaction Type: The type of transaction (e.g., online purchase, ATM withdrawal, POS transaction).\n",
        "* Fraudulent: Binary indicator (0 or 1) specifying whether the transaction is fraudulent (1) or not (0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "1bc1ec81",
      "metadata": {
        "id": "1bc1ec81",
        "outputId": "bfcd40bf-42e7-4584-98d0-6101afc15293",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F2 Score: 0.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, fbeta_score\n",
        "\n",
        "# Creating the dataset\n",
        "data = {\n",
        "    'Transaction ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10,\n",
        "                       11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n",
        "                       21, 22, 23, 24, 25, 26, 27, 28, 29, 30,\n",
        "                       31, 32, 33, 34, 35, 36, 37, 38, 39, 40],\n",
        "    'Transaction Amount': [1000, 500, 2000, 1500, 800, 3000, 1200, 700, 1800, 1300,\n",
        "                           900, 400, 2200, 1600, 850, 2800, 1100, 600, 1900, 1400,\n",
        "                           950, 300, 2100, 1700, 820, 3200, 1250, 720, 1850, 1350,\n",
        "                           880, 420, 2400, 1750, 830, 3100, 1150, 620, 1950, 1450],\n",
        "    'Merchant ID': ['M123', 'M456', 'M789', 'M123', 'M456', 'M789', 'M123', 'M456', 'M789', 'M123',\n",
        "                    'M456', 'M789', 'M123', 'M456', 'M789', 'M123', 'M456', 'M789', 'M123', 'M456',\n",
        "                    'M123', 'M456', 'M789', 'M123', 'M456', 'M789', 'M123', 'M456', 'M789', 'M123',\n",
        "                    'M456', 'M789', 'M123', 'M456', 'M789', 'M123', 'M456', 'M789', 'M123', 'M456'],\n",
        "    'Transaction Type': ['Online Purchase', 'ATM Withdrawal', 'Online Purchase', 'POS Transaction', 'Online Purchase',\n",
        "                         'ATM Withdrawal', 'Online Purchase', 'ATM Withdrawal', 'Online Purchase', 'POS Transaction',\n",
        "                         'Online Purchase', 'ATM Withdrawal', 'Online Purchase', 'ATM Withdrawal', 'Online Purchase',\n",
        "                         'POS Transaction', 'Online Purchase', 'ATM Withdrawal', 'Online Purchase', 'ATM Withdrawal',\n",
        "                         'Online Purchase', 'ATM Withdrawal', 'Online Purchase', 'POS Transaction', 'Online Purchase',\n",
        "                         'ATM Withdrawal', 'Online Purchase', 'ATM Withdrawal', 'Online Purchase', 'POS Transaction',\n",
        "                         'Online Purchase', 'ATM Withdrawal', 'Online Purchase', 'ATM Withdrawal', 'Online Purchase',\n",
        "                         'POS Transaction', 'Online Purchase', 'ATM Withdrawal', 'Online Purchase', 'ATM Withdrawal'],\n",
        "    'Fraudulent': [0, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
        "                   0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
        "                   1, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
        "                   0, 0, 1, 0, 0, 1, 0, 0, 1, 0]\n",
        "}\n",
        "\n",
        "# Creating DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# One-hot encoding for categorical features\n",
        "df = pd.get_dummies(df, columns=['Merchant ID', 'Transaction Type'], drop_first=True)\n",
        "\n",
        "# Splitting the dataset\n",
        "X = df.drop(['Fraudulent', 'Transaction ID'], axis=1)\n",
        "y = df['Fraudulent']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model training\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluation metrics\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f2_score = fbeta_score(y_test, y_pred, beta=2)\n",
        "\n",
        "# Printing the results\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F2 Score:\", f2_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4eb158cb",
      "metadata": {
        "id": "4eb158cb"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}