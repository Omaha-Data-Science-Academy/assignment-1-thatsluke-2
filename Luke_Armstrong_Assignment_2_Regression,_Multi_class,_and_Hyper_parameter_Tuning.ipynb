{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "705ccee2",
      "metadata": {
        "id": "705ccee2"
      },
      "source": [
        "# Assignment 2: Regression, Multi-class, and Hyper-parameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e81dbbc",
      "metadata": {
        "id": "8e81dbbc"
      },
      "source": [
        "### Task 1: Regression Metrics (30 points total)\n",
        "\n",
        "The code below executes the following steps:\n",
        "* Load the California Housing dataset from sklearn.\n",
        "* Split the dataset into training and testing sets.\n",
        "* Train a linear regression model on the training data.\n",
        "\n",
        "It is your task to:\n",
        "* Evaluate the model's performance using Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared metrics.\n",
        "* Print the evaluation results.\n",
        "* Interpret the results and discuss how each metric reflects the performance of a regression model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5a631ea7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a631ea7",
        "outputId": "10ca9392-081c-4aeb-bc94-f3f88ad744e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error (MAE): 0.533200130495698\n",
            "Mean Squared Error (MSE): 0.5558915986952422\n",
            "R-squared (R2): 0.5757877060324524\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Load the Boston Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate model performance\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print results so we can intepret\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "print(f\"R-squared (R2): {r2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ca0fcbb",
      "metadata": {
        "id": "5ca0fcbb"
      },
      "source": [
        "**Question: Interpret the results. How might we interpret the model performance and communicate it to stakeholders? (20 points)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e577b164",
      "metadata": {
        "id": "e577b164"
      },
      "source": [
        "*Your Answer:*\n",
        "Interpretation of Results\n",
        "Mean Absolute Error (MAE): 0.53\n",
        "MAE represents the average absolute difference between the predicted and actual values. In this case, on average, the model's predictions are off by about 0.53 units. This gives a sense of the typical magnitude of error.\n",
        "Mean Squared Error (MSE): 0.56\n",
        "MSE is similar to MAE but squares the errors before averaging them. This gives more weight to larger errors. An MSE of 0.56 means the average squared difference between predictions and actual values is 0.56. It's useful for understanding the variance of errors.\n",
        "R-squared (R2): 0.58\n",
        "R-squared indicates the proportion of variance in the target variable that is explained by the model. An R-squared of 0.58 means that about 58% of the variability in the target variable is captured by the model's predictions. The remaining 42% is unexplained."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ebf001c",
      "metadata": {
        "id": "5ebf001c"
      },
      "source": [
        "### Task 2: Multiclass Classification Metrics (30 points total)\n",
        "\n",
        "The code below executes the following steps:\n",
        "* Load the Iris dataset from scikit-learn.\n",
        "* Split the dataset into training and testing sets.\n",
        "* Train a multiclass classification model, logistic regression, on the training data.\n",
        "\n",
        "It is your task to:\n",
        "* Evaluate the model's performance using precision, recall, F1 score\n",
        "* Visualize a confusion matrix.\n",
        "* Print the evaluation results.\n",
        "* Interpret the results and discuss how each metric reflects the performance of a regression model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bd7ba5a2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd7ba5a2",
        "outputId": "590a60c1-df91-42cc-d726-d5c6e22d17ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1 Score: 1.0\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate model performance\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Print results so we can intepret\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "\n",
        "# Visualize a confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f37be54",
      "metadata": {
        "id": "7f37be54"
      },
      "source": [
        "**Question: Interpret the results. How might we interpret the model performance and communicate it to stakeholders? (20 points)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34b801e0",
      "metadata": {
        "id": "34b801e0"
      },
      "source": [
        "*Your Answer:*\n",
        "\n",
        "Interpretation of Results\n",
        "Precision: 1.0\n",
        "\n",
        "Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. A precision of 1.0 means that for each class, every prediction made by the model was correct (no false positives).\n",
        "Recall: 1.0\n",
        "\n",
        "Recall is the ratio of correctly predicted positive observations to all actual positive observations. A recall of 1.0 means the model correctly identified all actual positive instances for each class (no false negatives).\n",
        "F1 Score: 1.0\n",
        "\n",
        "The F1 score is the harmonic mean of precision and recall, providing a balance between the two. An F1 score of 1.0 indicates perfect precision and recall.\n",
        "Confusion Matrix:\n",
        "\n",
        "The confusion matrix visually represents the performance of a classification model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acd0e7e8",
      "metadata": {
        "id": "acd0e7e8"
      },
      "source": [
        "### Task 3: Model Selection, Hyperparameter Tuning, and Cross-Validation (40 points total)\n",
        "The code below executes the following steps:\n",
        "* Load in the Iris dataset.\n",
        "* Split into training and testing\n",
        "\n",
        "It is your task to:\n",
        "* Implement a grid search with cross-validation to tune hyperparameters for a classification model (e.g. random forest).\n",
        "* Explore different hyperparameters (e.g. number of estimators for random forest).\n",
        "* Evaluate the model's performance using accuracy, precision, recall, and F1 score on the testing set.\n",
        "* Print the **best hyperparameters** and evaluation results."
      ]
    },
    {
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score # Import cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Perform grid search with cross-validation\n",
        "cross_val_score(grid_search, X_train, y_train, cv=5) # Now cross_val_score is defined\n",
        "\n",
        "# Get the best hyperparameters\n",
        "hyperparameters = grid_search.best_params_\n",
        "\n",
        "# Evaluate model performance\n",
        "y_pred = grid_search.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the best hyperparameters and evaluation results\n",
        "print(\"Best Hyperparameters:\", hyperparameters)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViXoXEgv0n2O",
        "outputId": "2893b415-1e98-4db9-9c15-818f8627b1bc"
      },
      "id": "ViXoXEgv0n2O",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 1, 'kernel': 'linear'}\n",
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bd1f4fa",
      "metadata": {
        "id": "9bd1f4fa"
      },
      "source": [
        "### OPTIONAL Task 4: Custom Scoring Metric (20 bonus points)\n",
        "\n",
        "In sklearn, you are not limited to using their scoring functions. You can create your own!\n",
        "\n",
        "You can create a custom scoring metric in scikit-learn by defining a scoring function and then using the `make_scorer` function to wrap it as a scorer.\n",
        "\n",
        "**For bonus points:**\n",
        "\n",
        "* Define a custom scoring function custom_scoring that calculates the weighted sum of precision and recall for a binary classification problem.\n",
        "* Then wrap this function using make_scorer to create a custom scorer custom_scorer.\n",
        "* Use this custom scorer in cross-validation to evaluate the performance of a logistic regression model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cbb03387",
      "metadata": {
        "id": "cbb03387"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define your custom scoring function\n",
        "def custom_scoring(y_true, y_pred, precision_weight = 0.6, recall_weight = 0.4):\n",
        "    # calculate the weighted sum of prescision and recall for binary classification\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    score = precision_weight * precision + recall_weight * recall\n",
        "\n",
        "    return score\n",
        "\n",
        "# Wrap the custom scoring function as a scorer\n",
        "custom_scorer = make_scorer(custom_scoring, greater_is_better=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "46bbb6ae",
      "metadata": {
        "id": "46bbb6ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c075ce06-59a0-48e8-e9d3-0b191f85af05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Scores: [0.9        1.         0.85333333 0.94545455 0.96      ]\n",
            "Mean Custom Score: 0.9317575757575757\n"
          ]
        }
      ],
      "source": [
        "# THIS CODE TESTS YOUR FUNCTION\n",
        "# Generate sample data\n",
        "X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n",
        "\n",
        "# Create and train a model using cross-validation\n",
        "model = LogisticRegression()\n",
        "scores = cross_val_score(model, X, y, cv=5, scoring=custom_scorer)\n",
        "\n",
        "# Print the custom scores obtained from cross-validation\n",
        "print(\"Custom Scores:\", scores)\n",
        "print(\"Mean Custom Score:\", scores.mean())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}